nameOverride: ""
fullnameOverride: ""

image:
  pullSecrets: []
  listenerOperator:
    repository: quay.io/zncdatadev/listener-operator
    tag: "" # Defaults to chart appVersion
    pullPolicy: IfNotPresent
  csiDriver:
    repository: quay.io/zncdatadev/listener-csi-driver
    tag: "" # Defaults to chart appVersion
    pullPolicy: IfNotPresent
  csiProvisioner:
    repository: quay.io/zncdatadev/sig-storage/csi-provisioner
    tag: v5.1.0
    pullPolicy: IfNotPresent
  csiNodeDriverRegistrar:
    repository: quay.io/zncdatadev/sig-storage/csi-node-driver-registrar
    tag: v2.12.0
    pullPolicy: IfNotPresent
  livenessProbe:
    repository: quay.io/zncdatadev/sig-storage/livenessprobe
    tag: v2.14.0
    pullPolicy: IfNotPresent

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# Kubelet dir may vary in environments such as microk8s
kubeletDir: /var/lib/kubelet

csiController:
  logLevel: 2
  # Number of controller replicas
  replicas: 1
  # Resources of the CSI controller container
  ## resources:
  ##   requests:
  ##     cpu: 10m
  ##     memory: 16Mi
  ##   limits:
  ##     cpu: 100m
  ##     memory: 64Mi
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # Security context for the CSI controller container
  securityContext:
    capabilities:
      drop:
      - ALL

csiNode:
  logLevel: 2
  # Resources of the CSI node container
  ## resources:
  ##   requests:
  ##     cpu: 10m
  ##     memory: 16Mi
  ##   limits:
  ##     cpu: 100m
  ##     memory: 64Mi
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # Security context for the CSI node container
  securityContext:
    runAsUser: 0
    # When mount with Bidirectional propagation, the container must provide privileged access
    privileged: true
    allowPrivilegeEscalation: true
    capabilities:
      add: ["SYS_ADMIN"]
      drop:
      - ALL

csiProvisioner:
  logLevel: 2
  ## resources:
  ##   requests:
  ##     cpu: 10m
  ##     memory: 16Mi
  ##   limits:
  ##     cpu: 100m
  ##     memory: 64Mi
  resources: {}
csiNodeDriverRegistrar:
  logLevel: 2
  ## resources:
  ##   requests:
  ##     cpu: 10m
  ##     memory: 16Mi
  ##   limits:
  ##     cpu: 50m
  ##     memory: 32Mi
  resources: {}
livenessProbe:
  logLevel: 2
  ## resources:
  ##   requests:
  ##     cpu: 10m
  ##     memory: 16Mi
  ##   limits:
  ##     cpu: 50m
  ##     memory: 16Mi
  resources: {}


podSecurityContext:
  # privileged: true
  # runAsNonRoot: true
  # runAsUser: 1000
  seccompProfile:
    type: RuntimeDefault

controller:
  replicas: 1
  logLevel: 2
  ## resources:
  ##   # Resource requests and limits for the controller pod
  ##   requests:
  ##     cpu: 10m
  ##     memory: 16Mi
  ##   limits:
  ##     cpu: 100m
  ##     memory: 64Mi
  resources: {}

podAnnotations: {}

labels: {}

nodeSelector: {}

tolerations: []

affinity: {}

# Options: none, stable-nodes, ephemeral-nodes
# none: No ListenerClasses are preinstalled, the administrator must supply them themself
# stable-nodes: ListenerClasses are preinstalled that are suitable for on-prem/"pet" environments, assuming long-running Nodes but not requiring a LoadBalancer controller
# ephemeral-nodes: ListenerClasses are preinstalled that are suitable for cloud/"cattle" environments with short-lived nodes, however this requires a LoadBalancer controller to be installed
preset: stable-nodes


# DaemonSet post-install check
postInstall:
  checkDaemonSet:
    enabled: true
    image: "quay.io/zncdatadev/tools:1.0.0-kubedoop0.0.0-dev"
    attempts: 30
    sleepTime: 10
    backoffLimit: 6
    ttlSecondsAfterFinished: 100
    nodeSelector: {}
    tolerations: []
    # Minimum number of available nodes for the DaemonSet to be considered ready
    # If set to 0, the DaemonSet is considered ready if it is running on at all nodes
    minAvailableNodes: 0
    resources: {}
      # requests:
      #   cpu: 100m
      #   memory: 64Mi
      # limits:
      #   cpu: 200m
      #   memory: 128Mi
